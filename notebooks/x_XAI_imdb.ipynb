{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from captum.attr import IntegratedGradients, LayerIntegratedGradients\n",
    "from captum.attr import visualization\n",
    "\n",
    "import shap\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI models needs to use a pre-trained model\n",
    "\n",
    "# either train one your self on the IMDB dataset or find one pre-trained on huggingface!\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/bert_imdb.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(BertModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids):        \n",
    "        outputs = self.model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        return nn.functional.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_ref(model_wrapper, sentence):\n",
    "    input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)], device=device)\n",
    "    \n",
    "    ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "    sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "    cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "    \n",
    "    ref_input_ids = [cls_token_id] + (input_ids.size(1)-2) * [ref_token_id] + [sep_token_id]\n",
    "    ref_input_ids = torch.tensor([ref_input_ids], device=device)\n",
    "    \n",
    "    return input_ids, ref_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attributions_to_visualizer(attributions, tokens, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    #attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.detach().cpu().clone().numpy()\n",
    "    \n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions/np.linalg.norm(attributions),\n",
    "                            pred,\n",
    "                            pred_ind,\n",
    "                            label,\n",
    "                            \"label\",\n",
    "                            attributions.sum(),       \n",
    "                            tokens[:len(attributions)],\n",
    "                            delta))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_wrapper = BertModelWrapper(model)\n",
    "lig = LayerIntegratedGradients(bert_model_wrapper, bert_model_wrapper.model.bert.embeddings)\n",
    "\n",
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "bert_model_wrapper.eval()\n",
    "bert_model_wrapper.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = np.random.choice(len(test_dataset))\n",
    "r=10\n",
    "label = test_dataset['label'][r].item()\n",
    "sentence = test_dataset['text'][r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids = input_ref(bert_model_wrapper, sentence)\n",
    "\n",
    "pred = bert_model_wrapper(input_ids)[:, 1].unsqueeze(1).item()\n",
    "pred_ind = round(pred)\n",
    "\n",
    "attributions, delta = lig.attribute(inputs=input_ids, n_steps=500,\n",
    "                                   baselines=ref_input_ids,\n",
    "                                    internal_batch_size=32,\n",
    "                                    return_convergence_delta=True,\n",
    "                                    target=pred_ind\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pred: ', pred_ind, '(', '%.2f' % pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().clone().numpy().tolist())    \n",
    "add_attributions_to_visualizer(attributions, tokens, pred, pred_ind, label, delta, vis_data_records_ig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=512,truncation=True) for v in x]).cuda()    \n",
    "    attention_mask = (tv!=0).type(torch.int64).cuda()\n",
    "    outputs = model(tv,attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = model.config.id2label\n",
    "label2id = model.config.label2id\n",
    "labels = sorted(label2id, key=label2id.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "shap_values = explainer([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[:,:,'LABEL_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.choice(len(test_dataset))\n",
    "label = test_dataset['label'][r].item()\n",
    "sentence = test_dataset['text'][r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-amazon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
