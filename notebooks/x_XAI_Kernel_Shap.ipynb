{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import shap\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(input_ids, attention_mask=None, batch_size=32, label=None,\n",
    "               output_logits=False, repeat_input_ids=False):\n",
    "    \"\"\"\n",
    "    Wrapper function for a Huggingface Transformers model into the format that KernelSHAP expects,\n",
    "    i.e. where inputs and outputs are numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.ones_like(input_ids) if attention_mask is None else torch.tensor(attention_mask)\n",
    "\n",
    "    if repeat_input_ids:\n",
    "        assert input_ids.shape[0] == 1\n",
    "        input_ids = input_ids.repeat(attention_mask.shape[0], 1)\n",
    " \n",
    "    ds = torch.utils.data.TensorDataset(input_ids.long(), attention_mask.long())\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    probas = []\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            out = model(batch[0], attention_mask=batch[1])\n",
    "            logits.append(out[0].detach())\n",
    "            probas.append(torch.nn.functional.softmax(out[0],\n",
    "                                                      dim=1).detach())\n",
    "    logits = torch.cat(logits, dim=0).numpy()\n",
    "    probas = torch.cat(probas, dim=0).numpy()\n",
    "\n",
    "    if label is not None:\n",
    "        probas = probas[:, label]\n",
    "        logits = logits[:, label]\n",
    "\n",
    "    return (probas, logits) if output_logits else probas\n",
    "\n",
    "def tokens2words(tokens, seq, token_prefix=\"##\"):\n",
    "    \"\"\"\n",
    "    Utility function to aggregate 'seq' on word-level based on 'tokens'\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = []\n",
    "    for token, x in zip(tokens, seq):\n",
    "        if token.startswith(token_prefix):\n",
    "            if type(x) == str:\n",
    "                x = x.replace(token_prefix,\"\")\n",
    "            tmp[-1] += x\n",
    "        else:\n",
    "            if type(x) == str:\n",
    "                tmp.append(x)\n",
    "            else:\n",
    "                tmp.append(x.item())\n",
    "\n",
    "    return tmp if type(tmp[-1]) == str else torch.tensor(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 500\n",
    "idx = 101\n",
    "ref_token = tokenizer.mask_token_id # Could also consider <UNK> or <PAD> tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = sst2_dataset[\"validation\"][idx][\"sentence\"]\n",
    "label = sst2_dataset[\"validation\"][idx][\"label\"]\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"np\")\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "input_words = tokens2words(input_tokens, input_tokens)\n",
    "pred = predict_fn(input_ids)\n",
    "pred_label = pred.argmax()\n",
    "pred_p = pred[0, pred_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = input_ids.copy()\n",
    "baseline_attn = np.zeros_like(input_ids)\n",
    "\n",
    "# Keep CLS and SEP tokens fixed in baseline\n",
    "baseline[:, 1:-1] = ref_token\n",
    "baseline_attn[:, 0] = 1\n",
    "baseline_attn[:, -1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn_label = functools.partial(predict_fn, label=pred_label)\n",
    "predict_fn_label_attn = functools.partial(predict_fn_label, input_ids, repeat_input_ids=True)\n",
    "\n",
    "explainer = shap.KernelExplainer(predict_fn_label, baseline)\n",
    "explainer_attn = shap.KernelExplainer(predict_fn_label_attn, baseline_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = explainer.shap_values(input_ids, nsamples=nsamples)\n",
    "phi_words = tokens2words(input_tokens, phi.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_rec = [viz.VisualizationDataRecord(\n",
    "    phi_words/phi_words.norm(), pred_p, pred_label, label,\n",
    "    pred_label, phi_words.sum(), input_words, None)]\n",
    "\n",
    "phi_attn = explainer_attn.shap_values(np.ones_like(input_ids), nsamples=nsamples)\n",
    "phi_attn_words = tokens2words(input_tokens, phi_attn.squeeze())\n",
    "viz_rec_attn = [viz.VisualizationDataRecord(\n",
    "    phi_attn_words/phi_attn_words.norm(), pred_p, pred_label, label,\n",
    "    pred_label, phi_attn_words.sum(), input_words, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_text(viz_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_text(viz_rec_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-tutorial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
